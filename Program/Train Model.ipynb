{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "######################################## Library for CNN ########################################\n",
    "from tensorflow.keras import Model, models\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.applications import vgg16, resnet50, mobilenet, densenet, xception\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "######################################## General Library ########################################\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import json as js\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "######################################### Plot Library #########################################\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import export_png\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.palettes import cividis\n",
    "from bokeh.layouts import gridplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCH = 30\n",
    "IMAGE_SIZE = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 10\n",
    "CUT_AT = 42\n",
    "UNFREEZE_FROM = 0\n",
    "BASE_ARCH = 'Xception'\n",
    "\n",
    "TRAINING_DATASET = os.path.join('japanese_fagaceae_dataset_cropped_'+str(IMAGE_SIZE), 'train')\n",
    "VALIDATION_DATASET = os.path.join('japanese_fagaceae_dataset_cropped_'+str(IMAGE_SIZE), 'validation')\n",
    "\n",
    "CLASSES = len(os.listdir(TRAINING_DATASET))\n",
    "\n",
    "OUTPUT_PATH = ['result', str(IMAGE_SIZE), BASE_ARCH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BASE_ARCH == 'VGG16':\n",
    "    preprocessing_function = vgg16.preprocess_input\n",
    "elif BASE_ARCH == 'ResNet50':\n",
    "    preprocessing_function = resnet50.preprocess_input\n",
    "elif BASE_ARCH =='MobileNet':\n",
    "    preprocessing_function = mobilenet.preprocess_input\n",
    "elif BASE_ARCH =='DenseNet121':\n",
    "    preprocessing_function = densenet.preprocess_input\n",
    "elif BASE_ARCH =='Xception':\n",
    "    preprocessing_function = xception.preprocess_input\n",
    "else:\n",
    "    pass\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocessing_function\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocessing_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147600 images belonging to 9 classes.\n",
      "Found 22140 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAINING_DATASET,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DATASET,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHT = copy.copy(OUTPUT_PATH)\n",
    "MODEL_WEIGHT.append('['+BASE_ARCH+'_'+str(IMAGE_SIZE)+'] weight.h5')\n",
    "\n",
    "Checkpoint = ModelCheckpoint(os.path.sep.join(MODEL_WEIGHT), \n",
    "                               monitor='val_acc',\n",
    "                               save_best_only=True\n",
    "                              )\n",
    "EarlyStopping = EarlyStopping(monitor=\"val_acc\", \n",
    "                                patience=PATIENCE, \n",
    "                                mode=\"max\"\n",
    "                               )\n",
    "ReduceLR = ReduceLROnPlateau(monitor=\"val_acc\", \n",
    "                                 factor=0.6, \n",
    "                                 patience=5, \n",
    "                                 min_lr=1e-6, \n",
    "                                 verbose=1, \n",
    "                                 mode=\"max\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\taufi\\Anaconda3\\envs\\tf_14\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if BASE_ARCH == 'VGG16':\n",
    "    model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "elif BASE_ARCH == 'ResNet50':\n",
    "    model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "elif BASE_ARCH == 'MobileNet':\n",
    "    model = mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "elif BASE_ARCH == 'DenseNet121':\n",
    "    model = densenet.DenseNet121(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "elif BASE_ARCH == 'Xception':\n",
    "    model = xception.Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "else:\n",
    "    pass\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "if CUT_AT==-1:\n",
    "    x = model.output\n",
    "else:\n",
    "    x = model.layers[CUT_AT].output\n",
    "    \n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(CLASSES, activation='softmax')(x)\n",
    "\n",
    "# instantiate new model\n",
    "jf_model = Model(inputs=model.input, outputs=predictions, name='japanese_fagaceae_model')\n",
    "\n",
    "# unfreeze selected layer\n",
    "for layer in jf_model.layers[UNFREEZE_FROM:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# optimizer\n",
    "opt=Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# compile model\n",
    "jf_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=opt, \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x000001B875F6D048> True\n",
      "1 <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001B870E75388> True\n",
      "2 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B8702FD048> True\n",
      "3 <tensorflow.python.keras.layers.core.Activation object at 0x000001B875FAF9C8> True\n",
      "4 <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001B875F79748> True\n",
      "5 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B87609CFC8> True\n",
      "6 <tensorflow.python.keras.layers.core.Activation object at 0x000001B8760C5F88> True\n",
      "7 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B8761A8408> True\n",
      "8 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876264E08> True\n",
      "9 <tensorflow.python.keras.layers.core.Activation object at 0x000001B876281348> True\n",
      "10 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B8762BB7C8> True\n",
      "11 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876386348> True\n",
      "12 <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001B8760C5108> True\n",
      "13 <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001B876386948> True\n",
      "14 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B8761A0908> True\n",
      "15 <tensorflow.python.keras.layers.merge.Add object at 0x000001B8763D3108> True\n",
      "16 <tensorflow.python.keras.layers.core.Activation object at 0x000001B876490408> True\n",
      "17 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B876490308> True\n",
      "18 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876563288> True\n",
      "19 <tensorflow.python.keras.layers.core.Activation object at 0x000001B8765A1F88> True\n",
      "20 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B8765B91C8> True\n",
      "21 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876669E08> True\n",
      "22 <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001B876470548> True\n",
      "23 <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001B876673508> True\n",
      "24 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B87648A988> True\n",
      "25 <tensorflow.python.keras.layers.merge.Add object at 0x000001B8766BB188> True\n",
      "26 <tensorflow.python.keras.layers.core.Activation object at 0x000001B876789C88> True\n",
      "27 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B87677F388> True\n",
      "28 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876852FC8> True\n",
      "29 <tensorflow.python.keras.layers.core.Activation object at 0x000001B87685D508> True\n",
      "30 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B8768D8CC8> True\n",
      "31 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876959C48> True\n",
      "32 <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001B876748B08> True\n",
      "33 <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001B8769636C8> True\n",
      "34 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B87677A548> True\n",
      "35 <tensorflow.python.keras.layers.merge.Add object at 0x000001B87699BA48> True\n",
      "36 <tensorflow.python.keras.layers.core.Activation object at 0x000001B876A30888> True\n",
      "37 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B876A5F208> True\n",
      "38 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876A64DC8> True\n",
      "39 <tensorflow.python.keras.layers.core.Activation object at 0x000001B876AA5FC8> True\n",
      "40 <tensorflow.python.keras.layers.convolutional.SeparableConv2D object at 0x000001B876AA5E48> True\n",
      "41 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B876B715C8> True\n",
      "42 <tensorflow.python.keras.layers.core.Activation object at 0x000001B875F78108> True\n",
      "43 <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x000001B87ECB9108> True\n",
      "44 <tensorflow.python.keras.layers.core.Dense object at 0x000001B877F102C8> True\n",
      "45 <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x000001B82B7C1348> True\n",
      "46 <tensorflow.python.keras.layers.core.Dropout object at 0x000001B82B7CB748> True\n",
      "47 <tensorflow.python.keras.layers.core.Dense object at 0x000001B82B7AC4C8> True\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(jf_model.layers):\n",
    "    print(str(i), layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] ['result', '100', 'Xception', '[Xception_100] model_arch.png']\n"
     ]
    }
   ],
   "source": [
    "PLOT_ARCH = copy.copy(OUTPUT_PATH)\n",
    "PLOT_ARCH.append('['+BASE_ARCH+'_'+str(IMAGE_SIZE)+'] model_arch.png')\n",
    "\n",
    "plot_model(\n",
    "    jf_model,\n",
    "    to_file=os.path.sep.join(PLOT_ARCH),\n",
    "    show_shapes=True,\n",
    "    show_layer_names=False\n",
    ")\n",
    "\n",
    "print('[SAVED]', PLOT_ARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4613/4613 [==============================] - 452s 98ms/step - loss: 0.4078 - acc: 0.8581 - val_loss: 0.8291 - val_acc: 0.8222\n",
      "Epoch 2/20\n",
      "4613/4613 [==============================] - 464s 101ms/step - loss: 0.1096 - acc: 0.9614 - val_loss: 0.4974 - val_acc: 0.8706\n",
      "Epoch 3/20\n",
      "4613/4613 [==============================] - 424s 92ms/step - loss: 0.0634 - acc: 0.9781 - val_loss: 0.3803 - val_acc: 0.8931\n",
      "Epoch 4/20\n",
      "4613/4613 [==============================] - 417s 90ms/step - loss: 0.0426 - acc: 0.9853 - val_loss: 0.4330 - val_acc: 0.8947\n",
      "Epoch 5/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0317 - acc: 0.9892 - val_loss: 0.5379 - val_acc: 0.8707\n",
      "Epoch 6/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0248 - acc: 0.9916 - val_loss: 0.7495 - val_acc: 0.8660\n",
      "Epoch 7/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0208 - acc: 0.9931 - val_loss: 0.6623 - val_acc: 0.8792\n",
      "Epoch 8/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0175 - acc: 0.9941 - val_loss: 0.4827 - val_acc: 0.8970\n",
      "Epoch 9/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0155 - acc: 0.9946 - val_loss: 0.3722 - val_acc: 0.9066\n",
      "Epoch 10/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0133 - acc: 0.9956 - val_loss: 0.5086 - val_acc: 0.8844\n",
      "Epoch 11/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.5630 - val_acc: 0.8886\n",
      "Epoch 12/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0112 - acc: 0.9963 - val_loss: 0.3395 - val_acc: 0.9292\n",
      "Epoch 13/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0101 - acc: 0.9968 - val_loss: 0.5723 - val_acc: 0.8713\n",
      "Epoch 14/20\n",
      "4613/4613 [==============================] - 417s 90ms/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.4733 - val_acc: 0.9047\n",
      "Epoch 15/20\n",
      "4613/4613 [==============================] - 417s 90ms/step - loss: 0.0087 - acc: 0.9973 - val_loss: 0.9796 - val_acc: 0.8410\n",
      "Epoch 16/20\n",
      "4613/4613 [==============================] - 417s 90ms/step - loss: 0.0091 - acc: 0.9971 - val_loss: 0.7596 - val_acc: 0.8528\n",
      "Epoch 17/20\n",
      "4612/4613 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 5.999999848427251e-05.\n",
      "4613/4613 [==============================] - 418s 91ms/step - loss: 0.0072 - acc: 0.9977 - val_loss: 0.4438 - val_acc: 0.9166\n",
      "Epoch 18/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.7405 - val_acc: 0.8683\n",
      "Epoch 19/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.7194 - val_acc: 0.8820\n",
      "Epoch 20/20\n",
      "4613/4613 [==============================] - 416s 90ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 1.2615 - val_acc: 0.8541\n",
      "\n",
      "\n",
      "training speed = 8421.03 seconds\n",
      "training speed = 140.35 minutes\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "history = jf_model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=EPOCH,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[Checkpoint, EarlyStopping, ReduceLR],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('\\n\\ntraining speed = %.2f seconds' % (toc-tic))\n",
    "print('training speed = %.2f minutes' % ((toc-tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] ['result', '100', 'Xception', '[Xception_100] weight.txt']\n"
     ]
    }
   ],
   "source": [
    "MODEL_LABEL = copy.copy(OUTPUT_PATH)\n",
    "MODEL_LABEL.append('['+BASE_ARCH+'_'+str(IMAGE_SIZE)+'] weight.txt')\n",
    "\n",
    "label2index = train_generator.class_indices\n",
    "idx2label = dict((v, [k]) for k, v in label2index.items())\n",
    "with open(os.path.sep.join(MODEL_LABEL), 'w') as f:\n",
    "    js.dump(idx2label, f)\n",
    "    \n",
    "print('[SAVED]', MODEL_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] ['result', '100', 'Xception', '[Xception_100] training_history.html']\n"
     ]
    }
   ],
   "source": [
    "PLOT_TRAIN_HISTORY = copy.copy(OUTPUT_PATH)\n",
    "PLOT_TRAIN_HISTORY.append('['+BASE_ARCH+'_'+str(IMAGE_SIZE)+'] training_history.html')\n",
    "output_file(os.path.sep.join(PLOT_TRAIN_HISTORY))\n",
    "\n",
    "palette =cividis(4)\n",
    "\n",
    "acc_source = ColumnDataSource(dict(\n",
    "    x=list(np.arange(1, len(history.history['acc'])+1)),\n",
    "    trainY=list(history.history['acc']),\n",
    "    validationY=list(history.history['val_acc']),\n",
    "))\n",
    "\n",
    "loss_source = ColumnDataSource(dict(\n",
    "    x=list(np.arange(1, len(history.history['acc'])+1)),\n",
    "    trainY=list(history.history['loss']),\n",
    "    validationY=list(history.history['val_loss']),\n",
    "))\n",
    "\n",
    "plot_acc = figure(title='Model Accuracy',\n",
    "                  x_axis_label='Epoch',\n",
    "                  y_axis_label='Accuracy')\n",
    "plot_acc.axis.axis_label_text_font_size='25pt'\n",
    "plot_acc.axis.major_label_text_font_size = '15pt'\n",
    "plot_acc.title.text_font_size ='25pt'\n",
    "\n",
    "plot_loss = figure(title='Model Loss',\n",
    "                  x_axis_label='Epoch',\n",
    "                  y_axis_label='Loss')\n",
    "plot_loss.axis.axis_label_text_font_size='25pt'\n",
    "plot_loss.axis.major_label_text_font_size = '15pt'\n",
    "plot_loss.title.text_font_size ='25pt'\n",
    "\n",
    "plot_acc.line(x='x', y='trainY',\n",
    "              color=palette[0],\n",
    "              legend_label='Train Accuracy',\n",
    "              source=acc_source)\n",
    "plot_acc.circle(x='x', y='trainY',\n",
    "                color=palette[1],\n",
    "                legend_label='Train Accuracy',\n",
    "                source=acc_source)\n",
    "plot_acc.line(x='x', y='validationY',\n",
    "              color=palette[2],\n",
    "              legend_label='Validation Accuracy',\n",
    "              source=acc_source)\n",
    "plot_acc.circle(x='x', y='validationY',\n",
    "                color=palette[3],\n",
    "                legend_label='Validation Accuracy',\n",
    "                source=acc_source)\n",
    "\n",
    "plot_loss.line(x='x', y='trainY',\n",
    "               color=palette[0],\n",
    "               legend_label='Train Loss',\n",
    "               source=loss_source)\n",
    "plot_loss.circle(x='x', y='trainY',\n",
    "                 color=palette[1],\n",
    "                 legend_label='Train Loss',\n",
    "                 source=loss_source)\n",
    "plot_loss.line(x='x', y='validationY',\n",
    "               color=palette[2],\n",
    "               legend_label='Validation Loss',\n",
    "               source=loss_source)\n",
    "plot_loss.circle(x='x', y='validationY',\n",
    "                 color=palette[3],\n",
    "                 legend_label='Validation Loss',\n",
    "                 source=loss_source)\n",
    "\n",
    "plot_acc.legend.location = \"top_left\"\n",
    "plot_acc.legend.label_text_font_size = \"15pt\"\n",
    "plot_acc.legend.glyph_height = 30\n",
    "plot_acc.legend.glyph_width = 30\n",
    "plot_loss.legend.label_text_font_size = \"15pt\"\n",
    "plot_loss.legend.glyph_height = 30\n",
    "plot_loss.legend.glyph_width = 30\n",
    "\n",
    "grid = gridplot([plot_acc, plot_loss], ncols=2)\n",
    "\n",
    "show(grid)\n",
    "\n",
    "print('[SAVED]', PLOT_TRAIN_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
